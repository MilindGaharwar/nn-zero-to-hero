{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1993)   # For consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5\n",
    "\n",
    "f(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff275133580>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install graphviz\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "    \n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangular ('record') node for it\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            # if this value is a result of some operation, create an op node for it\n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's add this to value class\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self.grad = 0.0     # grad represents the derrivative of output (say L) wrt to value (say a or f)\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "            # this means the gradient is just copied as it is as \n",
    "            # we have seen it happening for addition            \n",
    "        \n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad          \n",
    "        \n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad          \n",
    "        \n",
    "        out._backward = _backward\n",
    "\n",
    "        return out \n",
    "\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        self.grad = 1.0 \n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently we cannot do integer addition directly, so, we will make change now\n",
    "# We will do this for multiply\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self.grad = 0.0     # grad represents the derrivative of output (say L) wrt to value (say a or f)\n",
    "        self._prev = set(_children)\n",
    "        self._backward = lambda: None\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)     # now, we can add integer directly\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "            # this means the gradient is just copied as it is as \n",
    "            # we have seen it happening for addition            \n",
    "        \n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad          \n",
    "        \n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other*self.data**(other - 1)) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def __rmul__(self, other):  #other * self\n",
    "        return self * other\n",
    "\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other**-1)\n",
    "\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)  \n",
    "    \n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), \"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad     # exp(x) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad          \n",
    "        \n",
    "        out._backward = _backward\n",
    "\n",
    "        return out \n",
    "\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        \n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=3.0)\n",
      "Value(data=3.0)\n",
      "Value(data=4.0)\n",
      "Value(data=1.0)\n",
      "Value(data=0.5)\n",
      "Value(data=16.0)\n",
      "Value(data=54.598150033144236)\n"
     ]
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(4.0)\n",
    "\n",
    "print(a+1)\n",
    "print(1+a)\n",
    "\n",
    "print(a*2)\n",
    "print(a/2)\n",
    "print(a/b)\n",
    "\n",
    "print(b**2)\n",
    "\n",
    "print(b.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Division**\n",
    "\n",
    "a / b\n",
    "\n",
    "a * (1/b)\n",
    "\n",
    "a * (b**-1)\n",
    "\n",
    "Let's make:\n",
    "x**k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(4.0)\n",
    "\n",
    "a/b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if everything works\n",
    "# inputs\n",
    "x1 = Value(2.0, label=\"x1\")\n",
    "x2 = Value(0.0, label=\"x2\")\n",
    "\n",
    "# Weights\n",
    "w1 = Value(-3.0, label=\"w1\")\n",
    "w2 = Value(1.0, label=\"w2\")\n",
    "\n",
    "# Bias of a neuron\n",
    "b = Value(6.88137358701954432, label=\"b\")\n",
    "\n",
    "# from figure we can see\n",
    "# x1*w1 + x2*w2 + b\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label =\"x1*w1\"\n",
    "x2w2 = x2*w2; x2w2.label =\"x2*w2\"\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = \"x1w1 + x2w2\"\n",
    "\n",
    "n = x1w1x2w2 + b; n.label = \"n\" # Without activation function now\n",
    "\n",
    "#o = n.tanh(); o.label = \"output\"\n",
    "# ---------- tanh break-up ----------\n",
    "e = (2*n).exp()\n",
    "o = (e - 1) / (e + 1); o.label = \"output\"\n",
    "\n",
    "# ---------- tanh break-up ----------\n",
    "\n",
    "\n",
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"3456pt\" height=\"210pt\"\n viewBox=\"0.00 0.00 3455.60 210.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 206)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-206 3451.6,-206 3451.6,4 -4,4\"/>\n<!-- 140679288691712 -->\n<g id=\"node1\" class=\"node\">\n<title>140679288691712</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"3177.6,-80.5 3177.6,-116.5 3447.6,-116.5 3447.6,-80.5 3177.6,-80.5\"/>\n<text text-anchor=\"middle\" x=\"3209.6\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\">output</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"3241.6,-80.5 3241.6,-116.5 \"/>\n<text text-anchor=\"middle\" x=\"3292.6\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.7071</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"3343.6,-80.5 3343.6,-116.5 \"/>\n<text text-anchor=\"middle\" x=\"3395.6\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.0000</text>\n</g>\n<!-- 140679288691712* -->\n<g id=\"node2\" class=\"node\">\n<title>140679288691712*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"3114.6\" cy=\"-98.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"3114.6\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 140679288691712*&#45;&gt;140679288691712 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140679288691712*&#45;&gt;140679288691712</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3141.62,-98.5C3149.12,-98.5 3157.87,-98.5 3167.37,-98.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3167.52,-102 3177.52,-98.5 3167.52,-95 3167.52,-102\"/>\n</g>\n<!-- 140679288689696 -->\n<g id=\"node3\" class=\"node\">\n<title>140679288689696</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1583,-81.5 1583,-117.5 1810,-117.5 1810,-81.5 1583,-81.5\"/>\n<text text-anchor=\"middle\" x=\"1593.5\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1604,-81.5 1604,-117.5 \"/>\n<text text-anchor=\"middle\" x=\"1655\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 1.7627</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1706,-81.5 1706,-117.5 \"/>\n<text text-anchor=\"middle\" x=\"1758\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.2500</text>\n</g>\n<!-- 140679288689552exp -->\n<g id=\"node26\" class=\"node\">\n<title>140679288689552exp</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1873.3\" cy=\"-99.5\" rx=\"27.1\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1873.3\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">exp</text>\n</g>\n<!-- 140679288689696&#45;&gt;140679288689552exp -->\n<g id=\"edge24\" class=\"edge\">\n<title>140679288689696&#45;&gt;140679288689552exp</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1810.18,-99.5C1819.26,-99.5 1827.95,-99.5 1835.79,-99.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1835.93,-103 1845.93,-99.5 1835.93,-96 1835.93,-103\"/>\n</g>\n<!-- 140679288689696* -->\n<g id=\"node4\" class=\"node\">\n<title>140679288689696*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1520\" cy=\"-99.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1520\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 140679288689696*&#45;&gt;140679288689696 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140679288689696*&#45;&gt;140679288689696</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1547.12,-99.5C1554.67,-99.5 1563.45,-99.5 1572.9,-99.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1572.95,-103 1582.95,-99.5 1572.95,-96 1572.95,-103\"/>\n</g>\n<!-- 140679288695840 -->\n<g id=\"node5\" class=\"node\">\n<title>140679288695840</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"774,-82.5 774,-118.5 1099,-118.5 1099,-82.5 774,-82.5\"/>\n<text text-anchor=\"middle\" x=\"831\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">x1w1 + x2w2</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"888,-82.5 888,-118.5 \"/>\n<text text-anchor=\"middle\" x=\"941.5\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"995,-82.5 995,-118.5 \"/>\n<text text-anchor=\"middle\" x=\"1047\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 140679288695984+ -->\n<g id=\"node12\" class=\"node\">\n<title>140679288695984+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1162\" cy=\"-72.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1162\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 140679288695840&#45;&gt;140679288695984+ -->\n<g id=\"edge16\" class=\"edge\">\n<title>140679288695840&#45;&gt;140679288695984+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1081.26,-82.49C1097.41,-80.47 1112.59,-78.56 1125.28,-76.97\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1125.84,-80.43 1135.32,-75.72 1124.96,-73.49 1125.84,-80.43\"/>\n</g>\n<!-- 140679288695840+ -->\n<g id=\"node6\" class=\"node\">\n<title>140679288695840+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"711\" cy=\"-100.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"711\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 140679288695840+&#45;&gt;140679288695840 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140679288695840+&#45;&gt;140679288695840</title>\n<path fill=\"none\" stroke=\"black\" d=\"M738.08,-100.5C745.51,-100.5 754.21,-100.5 763.74,-100.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"763.95,-104 773.95,-100.5 763.95,-97 763.95,-104\"/>\n</g>\n<!-- 140679288687680 -->\n<g id=\"node7\" class=\"node\">\n<title>140679288687680</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2.5,-55.5 2.5,-91.5 246.5,-91.5 246.5,-55.5 2.5,-55.5\"/>\n<text text-anchor=\"middle\" x=\"21.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">w2</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"40.5,-55.5 40.5,-91.5 \"/>\n<text text-anchor=\"middle\" x=\"91.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 1.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"142.5,-55.5 142.5,-91.5 \"/>\n<text text-anchor=\"middle\" x=\"194.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0000</text>\n</g>\n<!-- 140679288698144* -->\n<g id=\"node20\" class=\"node\">\n<title>140679288698144*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"312\" cy=\"-73.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"312\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 140679288687680&#45;&gt;140679288698144* -->\n<g id=\"edge17\" class=\"edge\">\n<title>140679288687680&#45;&gt;140679288698144*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.64,-73.5C256.7,-73.5 266.26,-73.5 274.79,-73.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"275,-77 285,-73.5 275,-70 275,-77\"/>\n</g>\n<!-- 140679288689840 -->\n<g id=\"node8\" class=\"node\">\n<title>140679288689840</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2561.6,-53.5 2561.6,-89.5 2788.6,-89.5 2788.6,-53.5 2561.6,-53.5\"/>\n<text text-anchor=\"middle\" x=\"2572.1\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2582.6,-53.5 2582.6,-89.5 \"/>\n<text text-anchor=\"middle\" x=\"2633.6\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 4.8284</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2684.6,-53.5 2684.6,-89.5 \"/>\n<text text-anchor=\"middle\" x=\"2736.6\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.1464</text>\n</g>\n<!-- 140679288689840&#45;&gt;140679288691712* -->\n<g id=\"edge19\" class=\"edge\">\n<title>140679288689840&#45;&gt;140679288691712*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2788.76,-78.45C2884.02,-84.33 3014.68,-92.4 3077.28,-96.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3077.24,-99.76 3087.44,-96.89 3077.68,-92.78 3077.24,-99.76\"/>\n</g>\n<!-- 140679288689840+ -->\n<g id=\"node9\" class=\"node\">\n<title>140679288689840+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"2231.6\" cy=\"-71.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"2231.6\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 140679288689840+&#45;&gt;140679288689840 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140679288689840+&#45;&gt;140679288689840</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2258.89,-71.5C2315.16,-71.5 2450.31,-71.5 2551.23,-71.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2551.5,-75 2561.5,-71.5 2551.5,-68 2551.5,-75\"/>\n</g>\n<!-- 140679288692912 -->\n<g id=\"node10\" class=\"node\">\n<title>140679288692912</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1227.5,-109.5 1227.5,-145.5 1454.5,-145.5 1454.5,-109.5 1227.5,-109.5\"/>\n<text text-anchor=\"middle\" x=\"1238\" y=\"-123.8\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1248.5,-109.5 1248.5,-145.5 \"/>\n<text text-anchor=\"middle\" x=\"1299.5\" y=\"-123.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1350.5,-109.5 1350.5,-145.5 \"/>\n<text text-anchor=\"middle\" x=\"1402.5\" y=\"-123.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.2203</text>\n</g>\n<!-- 140679288692912&#45;&gt;140679288689696* -->\n<g id=\"edge22\" class=\"edge\">\n<title>140679288692912&#45;&gt;140679288689696*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1454.6,-109.69C1464.87,-108.06 1474.68,-106.51 1483.4,-105.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1484.14,-108.56 1493.47,-103.54 1483.04,-101.64 1484.14,-108.56\"/>\n</g>\n<!-- 140679288695984 -->\n<g id=\"node11\" class=\"node\">\n<title>140679288695984</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1225,-54.5 1225,-90.5 1457,-90.5 1457,-54.5 1225,-54.5\"/>\n<text text-anchor=\"middle\" x=\"1238\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">n</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1251,-54.5 1251,-90.5 \"/>\n<text text-anchor=\"middle\" x=\"1302\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.8814</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1353,-54.5 1353,-90.5 \"/>\n<text text-anchor=\"middle\" x=\"1405\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 140679288695984&#45;&gt;140679288689696* -->\n<g id=\"edge20\" class=\"edge\">\n<title>140679288695984&#45;&gt;140679288689696*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1457.09,-90.06C1466.46,-91.49 1475.39,-92.85 1483.4,-94.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1482.96,-97.54 1493.37,-95.59 1484.01,-90.62 1482.96,-97.54\"/>\n</g>\n<!-- 140679288695984+&#45;&gt;140679288695984 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140679288695984+&#45;&gt;140679288695984</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1189.12,-72.5C1196.53,-72.5 1205.14,-72.5 1214.4,-72.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1214.69,-76 1224.69,-72.5 1214.69,-69 1214.69,-76\"/>\n</g>\n<!-- 140679288687824 -->\n<g id=\"node13\" class=\"node\">\n<title>140679288687824</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-165.5 0,-201.5 249,-201.5 249,-165.5 0,-165.5\"/>\n<text text-anchor=\"middle\" x=\"19\" y=\"-179.8\" font-family=\"Times,serif\" font-size=\"14.00\">w1</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"38,-165.5 38,-201.5 \"/>\n<text text-anchor=\"middle\" x=\"91.5\" y=\"-179.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;3.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"145,-165.5 145,-201.5 \"/>\n<text text-anchor=\"middle\" x=\"197\" y=\"-179.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.0000</text>\n</g>\n<!-- 140679288700160* -->\n<g id=\"node17\" class=\"node\">\n<title>140679288700160*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"312\" cy=\"-128.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"312\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 140679288687824&#45;&gt;140679288700160* -->\n<g id=\"edge12\" class=\"edge\">\n<title>140679288687824&#45;&gt;140679288700160*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M218.13,-165.46C228.63,-162.77 239.1,-159.78 249,-156.5 259.71,-152.96 271.05,-148.16 281.07,-143.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"282.67,-146.66 290.2,-139.21 279.67,-140.33 282.67,-146.66\"/>\n</g>\n<!-- 140679288688880 -->\n<g id=\"node14\" class=\"node\">\n<title>140679288688880</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2294.6,-108.5 2294.6,-144.5 2525.6,-144.5 2525.6,-108.5 2294.6,-108.5\"/>\n<text text-anchor=\"middle\" x=\"2305.1\" y=\"-122.8\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2315.6,-108.5 2315.6,-144.5 \"/>\n<text text-anchor=\"middle\" x=\"2366.6\" y=\"-122.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 6.8284</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2417.6,-108.5 2417.6,-144.5 \"/>\n<text text-anchor=\"middle\" x=\"2471.6\" y=\"-122.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad &#45;0.1036</text>\n</g>\n<!-- 140679288691568**&#45;1 -->\n<g id=\"node24\" class=\"node\">\n<title>140679288691568**&#45;1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"2675.1\" cy=\"-126.5\" rx=\"28.7\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"2675.1\" y=\"-122.8\" font-family=\"Times,serif\" font-size=\"14.00\">**&#45;1</text>\n</g>\n<!-- 140679288688880&#45;&gt;140679288691568**&#45;1 -->\n<g id=\"edge26\" class=\"edge\">\n<title>140679288688880&#45;&gt;140679288691568**&#45;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2525.69,-126.5C2565.38,-126.5 2607.17,-126.5 2636.25,-126.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2636.37,-130 2646.37,-126.5 2636.37,-123 2636.37,-130\"/>\n</g>\n<!-- 140679288688880+ -->\n<g id=\"node15\" class=\"node\">\n<title>140679288688880+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"2231.6\" cy=\"-126.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"2231.6\" y=\"-122.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 140679288688880+&#45;&gt;140679288688880 -->\n<g id=\"edge6\" class=\"edge\">\n<title>140679288688880+&#45;&gt;140679288688880</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2258.64,-126.5C2266.18,-126.5 2274.96,-126.5 2284.41,-126.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2284.49,-130 2294.49,-126.5 2284.49,-123 2284.49,-130\"/>\n</g>\n<!-- 140679288700160 -->\n<g id=\"node16\" class=\"node\">\n<title>140679288700160</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"375,-110.5 375,-146.5 648,-146.5 648,-110.5 375,-110.5\"/>\n<text text-anchor=\"middle\" x=\"406\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">x1*w1</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"437,-110.5 437,-146.5 \"/>\n<text text-anchor=\"middle\" x=\"490.5\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"544,-110.5 544,-146.5 \"/>\n<text text-anchor=\"middle\" x=\"596\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 140679288700160&#45;&gt;140679288695840+ -->\n<g id=\"edge14\" class=\"edge\">\n<title>140679288700160&#45;&gt;140679288695840+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M639.53,-110.49C652.09,-108.71 663.99,-107.02 674.3,-105.56\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"675.03,-108.99 684.44,-104.12 674.05,-102.06 675.03,-108.99\"/>\n</g>\n<!-- 140679288700160*&#45;&gt;140679288700160 -->\n<g id=\"edge7\" class=\"edge\">\n<title>140679288700160*&#45;&gt;140679288700160</title>\n<path fill=\"none\" stroke=\"black\" d=\"M339.23,-128.5C346.7,-128.5 355.41,-128.5 364.87,-128.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"364.98,-132 374.98,-128.5 364.98,-125 364.98,-132\"/>\n</g>\n<!-- 140679288690944 -->\n<g id=\"node18\" class=\"node\">\n<title>140679288690944</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"4.5,-0.5 4.5,-36.5 244.5,-36.5 244.5,-0.5 4.5,-0.5\"/>\n<text text-anchor=\"middle\" x=\"21.5\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">x2</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"38.5,-0.5 38.5,-36.5 \"/>\n<text text-anchor=\"middle\" x=\"89.5\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"140.5,-0.5 140.5,-36.5 \"/>\n<text text-anchor=\"middle\" x=\"192.5\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 140679288690944&#45;&gt;140679288698144* -->\n<g id=\"edge21\" class=\"edge\">\n<title>140679288690944&#45;&gt;140679288698144*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M218.13,-36.54C228.63,-39.23 239.1,-42.22 249,-45.5 259.71,-49.04 271.05,-53.84 281.07,-58.46\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"279.67,-61.67 290.2,-62.79 282.67,-55.34 279.67,-61.67\"/>\n</g>\n<!-- 140679288698144 -->\n<g id=\"node19\" class=\"node\">\n<title>140679288698144</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"377.5,-55.5 377.5,-91.5 645.5,-91.5 645.5,-55.5 377.5,-55.5\"/>\n<text text-anchor=\"middle\" x=\"408.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">x2*w2</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"439.5,-55.5 439.5,-91.5 \"/>\n<text text-anchor=\"middle\" x=\"490.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"541.5,-55.5 541.5,-91.5 \"/>\n<text text-anchor=\"middle\" x=\"593.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 140679288698144&#45;&gt;140679288695840+ -->\n<g id=\"edge18\" class=\"edge\">\n<title>140679288698144&#45;&gt;140679288695840+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M644.24,-91.51C655.12,-93 665.4,-94.4 674.45,-95.64\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"674.11,-99.13 684.49,-97.01 675.05,-92.19 674.11,-99.13\"/>\n</g>\n<!-- 140679288698144*&#45;&gt;140679288698144 -->\n<g id=\"edge8\" class=\"edge\">\n<title>140679288698144*&#45;&gt;140679288698144</title>\n<path fill=\"none\" stroke=\"black\" d=\"M339.23,-73.5C347.26,-73.5 356.72,-73.5 366.99,-73.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"367.08,-77 377.08,-73.5 367.08,-70 367.08,-77\"/>\n</g>\n<!-- 140679288686384 -->\n<g id=\"node21\" class=\"node\">\n<title>140679288686384</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2.5,-110.5 2.5,-146.5 246.5,-146.5 246.5,-110.5 2.5,-110.5\"/>\n<text text-anchor=\"middle\" x=\"19.5\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">x1</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"36.5,-110.5 36.5,-146.5 \"/>\n<text text-anchor=\"middle\" x=\"87.5\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"138.5,-110.5 138.5,-146.5 \"/>\n<text text-anchor=\"middle\" x=\"192.5\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad &#45;1.5000</text>\n</g>\n<!-- 140679288686384&#45;&gt;140679288700160* -->\n<g id=\"edge13\" class=\"edge\">\n<title>140679288686384&#45;&gt;140679288700160*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.64,-128.5C256.7,-128.5 266.26,-128.5 274.79,-128.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"275,-132 285,-128.5 275,-125 275,-132\"/>\n</g>\n<!-- 140679288686432 -->\n<g id=\"node22\" class=\"node\">\n<title>140679288686432</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"820.5,-27.5 820.5,-63.5 1052.5,-63.5 1052.5,-27.5 820.5,-27.5\"/>\n<text text-anchor=\"middle\" x=\"833.5\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"846.5,-27.5 846.5,-63.5 \"/>\n<text text-anchor=\"middle\" x=\"897.5\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 6.8814</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"948.5,-27.5 948.5,-63.5 \"/>\n<text text-anchor=\"middle\" x=\"1000.5\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 140679288686432&#45;&gt;140679288695984+ -->\n<g id=\"edge23\" class=\"edge\">\n<title>140679288686432&#45;&gt;140679288695984+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1052.52,-59.4C1078.82,-62.57 1104.87,-65.72 1124.85,-68.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1124.65,-71.63 1135,-69.36 1125.49,-64.68 1124.65,-71.63\"/>\n</g>\n<!-- 140679288691568 -->\n<g id=\"node23\" class=\"node\">\n<title>140679288691568</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2824.6,-106.5 2824.6,-142.5 3051.6,-142.5 3051.6,-106.5 2824.6,-106.5\"/>\n<text text-anchor=\"middle\" x=\"2835.1\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2845.6,-106.5 2845.6,-142.5 \"/>\n<text text-anchor=\"middle\" x=\"2896.6\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.1464</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2947.6,-106.5 2947.6,-142.5 \"/>\n<text text-anchor=\"middle\" x=\"2999.6\" y=\"-120.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 4.8284</text>\n</g>\n<!-- 140679288691568&#45;&gt;140679288691712* -->\n<g id=\"edge25\" class=\"edge\">\n<title>140679288691568&#45;&gt;140679288691712*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M3052.07,-107.67C3061.37,-106.28 3070.23,-104.96 3078.2,-103.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"3078.73,-107.23 3088.1,-102.3 3077.7,-100.31 3078.73,-107.23\"/>\n</g>\n<!-- 140679288691568**&#45;1&#45;&gt;140679288691568 -->\n<g id=\"edge9\" class=\"edge\">\n<title>140679288691568**&#45;1&#45;&gt;140679288691568</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2704.03,-126.29C2730.75,-126.08 2772.99,-125.76 2814.21,-125.44\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2814.4,-128.94 2824.37,-125.36 2814.35,-121.94 2814.4,-128.94\"/>\n</g>\n<!-- 140679288689552 -->\n<g id=\"node25\" class=\"node\">\n<title>140679288689552</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1939.1,-81.5 1939.1,-117.5 2166.1,-117.5 2166.1,-81.5 1939.1,-81.5\"/>\n<text text-anchor=\"middle\" x=\"1949.6\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1960.1,-81.5 1960.1,-117.5 \"/>\n<text text-anchor=\"middle\" x=\"2011.1\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 5.8284</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2062.1,-81.5 2062.1,-117.5 \"/>\n<text text-anchor=\"middle\" x=\"2114.1\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0429</text>\n</g>\n<!-- 140679288689552&#45;&gt;140679288689840+ -->\n<g id=\"edge11\" class=\"edge\">\n<title>140679288689552&#45;&gt;140679288689840+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2166.19,-81.69C2176.47,-80.06 2186.28,-78.51 2195,-77.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2195.73,-80.56 2205.06,-75.54 2194.64,-73.64 2195.73,-80.56\"/>\n</g>\n<!-- 140679288689552&#45;&gt;140679288688880+ -->\n<g id=\"edge27\" class=\"edge\">\n<title>140679288689552&#45;&gt;140679288688880+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2166.19,-116.68C2176.47,-118.24 2186.28,-119.74 2195,-121.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2194.65,-124.56 2205.06,-122.61 2195.7,-117.64 2194.65,-124.56\"/>\n</g>\n<!-- 140679288689552exp&#45;&gt;140679288689552 -->\n<g id=\"edge10\" class=\"edge\">\n<title>140679288689552exp&#45;&gt;140679288689552</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1900.84,-99.5C1908.97,-99.5 1918.49,-99.5 1928.74,-99.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1928.76,-103 1938.76,-99.5 1928.76,-96 1928.76,-103\"/>\n</g>\n<!-- 140679288689600 -->\n<g id=\"node27\" class=\"node\">\n<title>140679288689600</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1936.6,-26.5 1936.6,-62.5 2168.6,-62.5 2168.6,-26.5 1936.6,-26.5\"/>\n<text text-anchor=\"middle\" x=\"1947.1\" y=\"-40.8\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1957.6,-26.5 1957.6,-62.5 \"/>\n<text text-anchor=\"middle\" x=\"2011.1\" y=\"-40.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;1.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2064.6,-26.5 2064.6,-62.5 \"/>\n<text text-anchor=\"middle\" x=\"2116.6\" y=\"-40.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.1464</text>\n</g>\n<!-- 140679288689600&#45;&gt;140679288689840+ -->\n<g id=\"edge15\" class=\"edge\">\n<title>140679288689600&#45;&gt;140679288689840+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2168.69,-62.06C2178.05,-63.49 2186.98,-64.85 2195,-66.07\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2194.55,-69.54 2204.96,-67.59 2195.61,-62.62 2194.55,-69.54\"/>\n</g>\n<!-- 140679288689120 -->\n<g id=\"node28\" class=\"node\">\n<title>140679288689120</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1937.1,-136.5 1937.1,-172.5 2168.1,-172.5 2168.1,-136.5 1937.1,-136.5\"/>\n<text text-anchor=\"middle\" x=\"1947.6\" y=\"-150.8\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1958.1,-136.5 1958.1,-172.5 \"/>\n<text text-anchor=\"middle\" x=\"2009.1\" y=\"-150.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 1.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"2060.1,-136.5 2060.1,-172.5 \"/>\n<text text-anchor=\"middle\" x=\"2114.1\" y=\"-150.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad &#45;0.1036</text>\n</g>\n<!-- 140679288689120&#45;&gt;140679288688880+ -->\n<g id=\"edge28\" class=\"edge\">\n<title>140679288689120&#45;&gt;140679288688880+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M2167.69,-136.45C2177.48,-134.9 2186.8,-133.43 2195.14,-132.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"2195.79,-135.55 2205.12,-130.53 2194.69,-128.64 2195.79,-135.55\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7ff273079720>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs of this resembles exactly the output if we directly use tanh fucntion. So, it doesn't matter how we implement operations. \n",
    "\n",
    "All that matters is we can correctly calulate the forward and backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the same thing is PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())\n",
    "\n",
    "# The result is exactly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Net(s) now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w*x +b    # w*x is a dot product\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b )\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):       \n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "        # for neuron in self.neurons:\n",
    "        #     ps = neuron.parameters()\n",
    "        #     params.extend(ps)\n",
    "        # return params\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.007657120625740258)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_dot(n(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for training\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.007657120625740258),\n",
       " Value(data=0.9629698232963938),\n",
       " Value(data=0.6722963345252638),\n",
       " Value(data=-0.24414394993705751)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred =[n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=9.182464116048688)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We wil calculate the loss to measure the correcteness of the prediction: how well the neural net is performing\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the loss to be low\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17473429586587344"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].grad    # If the gradient is negative, that means by increasing the weight of this neuron, the loss will go down\n",
    "\n",
    "# We have this information for all of our neurons and their parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 grad 0.17473429586587344\n",
      "w1 data -0.03862055212227311\n"
     ]
    }
   ],
   "source": [
    "print('w1 grad', n.layers[0].neurons[0].w[0].grad)\n",
    "print('w1 data', n.layers[0].neurons[0].w[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will itirate through all the parameters\n",
    "\n",
    "for p in n.parameters():\n",
    "    p.data += -0.01 * p.grad    # if gradient is negative, we increase the weight, if it is positive, we decrease the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 grad 0.17473429586587344\n",
      "w1 data -0.040367895080931845\n"
     ]
    }
   ],
   "source": [
    "# after the change\n",
    "print('w1 grad', n.layers[0].neurons[0].w[0].grad)\n",
    "print('w1 data', n.layers[0].neurons[0].w[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=8.607967674232228)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we recalculate the loss   # we expecct it to be lower\n",
    "ypred =[n(x) for x in xs]\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss\n",
    "\n",
    "# It is lower\n",
    "\n",
    "# You won't get the exact same results as I did, but you should get a loss that is lower than the initial loss (it was randomly initialized) : \n",
    "# but now you will do because i seeded the random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will do backward pass\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in n.parameters():\n",
    "    p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=6.247012816219952)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred =[n(x) for x in xs]\n",
    "loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "loss\n",
    "\n",
    "# Now the loss should decrease more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is gradient descent, we are doing forward pass, backward pass and then update. As a result neural net is improving its predictions<>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.8009931947462453),\n",
       " Value(data=-0.6030405624037342),\n",
       " Value(data=0.7932559750217888),\n",
       " Value(data=0.7905048504993913)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now do this a few more time with an increased learning rate\n",
    "\n",
    "for e in range(5):\n",
    "    ypred =[n(x) for x in xs]\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "    loss.backward()\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "ypred\n",
    "\n",
    "# You can see the output is getting closer to the desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=3.4568357132496788)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the loss here is:\n",
    "loss\n",
    "\n",
    "# see it is very low now\n",
    "# loss = Value(data=3.1391947868174866e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the parameters\n",
    "# There are a set of trained neural net parameters that will give us the desired output\n",
    "\n",
    "#n.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do it in a proper way from begining like we do in regular trainings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step:  0 Loss:  8.590244752121581\n",
      "Current step:  2 Loss:  3.251279565120799\n",
      "Current step:  4 Loss:  0.4314166532029969\n",
      "Current step:  6 Loss:  0.008903970131844198\n",
      "Current step:  8 Loss:  0.00017030867419075724\n",
      "Current step:  10 Loss:  6.909809892444565e-06\n",
      "Current step:  12 Loss:  3.79564352360394e-07\n",
      "Current step:  14 Loss:  2.3918872973270876e-08\n",
      "Current step:  16 Loss:  1.6092511096719524e-09\n",
      "Current step:  18 Loss:  1.3445988501866946e-06\n",
      "[Value(data=0.9999971000178582), Value(data=-0.9999999999981305), Value(data=-0.999999999998144), Value(data=0.9999981856078193)]\n"
     ]
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "for e in range(20):\n",
    "    # forward pass\n",
    "    ypred =[n(x) for x in xs]\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -learning_rate * p.grad\n",
    "    \n",
    "    # print every\n",
    "    if e % 2 == 0:\n",
    "        print(\"Current step: \", e, \"Loss: \", loss.data)\n",
    "\n",
    "# After last step of training let's predict again\n",
    "ypred =[n(x) for x in xs]\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step:  0 Loss:  3.0671681764088587\n",
      "Current step:  2 Loss:  0.7522797144376534\n",
      "Current step:  4 Loss:  0.21049320414541034\n",
      "Current step:  6 Loss:  0.12931817259939735\n",
      "Current step:  8 Loss:  0.09173860850980789\n",
      "Current step:  10 Loss:  0.07046366092766423\n",
      "Current step:  12 Loss:  0.05690191664948921\n",
      "Current step:  14 Loss:  0.04755552992867856\n",
      "Current step:  16 Loss:  0.04074910543584072\n",
      "Current step:  18 Loss:  0.03558485523377747\n",
      "Current step:  20 Loss:  0.03154041675170391\n",
      "Current step:  22 Loss:  0.028292096705550128\n",
      "Current step:  24 Loss:  0.025629064823938723\n",
      "Current step:  26 Loss:  0.023408339828180122\n",
      "Current step:  28 Loss:  0.021529657545075617\n",
      "Current step:  30 Loss:  0.019920702795380423\n",
      "Current step:  32 Loss:  0.018528051621906523\n",
      "Current step:  34 Loss:  0.01731141003069202\n",
      "Current step:  36 Loss:  0.016239832977886108\n",
      "Current step:  38 Loss:  0.015289174466104255\n",
      "Current step:  40 Loss:  0.014440326294789117\n",
      "Current step:  42 Loss:  0.013677975530100375\n",
      "Current step:  44 Loss:  0.012989711231352619\n",
      "Current step:  46 Loss:  0.012365371303898295\n",
      "Current step:  48 Loss:  0.011796557581364723\n",
      "Current step:  50 Loss:  0.01127627078763775\n",
      "Current step:  52 Loss:  0.01079863225421862\n",
      "Current step:  54 Loss:  0.01035866931227851\n",
      "Current step:  56 Loss:  0.0099521480265217\n",
      "Current step:  58 Loss:  0.009575441547888152\n",
      "Current step:  60 Loss:  0.009225425560284873\n",
      "Current step:  62 Loss:  0.008899394546933826\n",
      "Current step:  64 Loss:  0.008594994206319886\n",
      "Current step:  66 Loss:  0.008310166505547124\n",
      "Current step:  68 Loss:  0.008043104703973538\n",
      "Current step:  70 Loss:  0.0077922163033184435\n",
      "Current step:  72 Loss:  0.007556092344763569\n",
      "Current step:  74 Loss:  0.00733348182266993\n",
      "Current step:  76 Loss:  0.00712327024928288\n",
      "Current step:  78 Loss:  0.006924461607232828\n",
      "Current step:  80 Loss:  0.0067361630826139815\n",
      "Current step:  82 Loss:  0.006557572092482956\n",
      "Current step:  84 Loss:  0.00638796521522437\n",
      "Current step:  86 Loss:  0.006226688706644633\n",
      "Current step:  88 Loss:  0.006073150343552104\n",
      "Current step:  90 Loss:  0.005926812383468344\n",
      "Current step:  92 Loss:  0.005787185466652096\n",
      "Current step:  94 Loss:  0.005653823316826649\n",
      "Current step:  96 Loss:  0.005526318121438399\n",
      "Current step:  98 Loss:  0.005404296492137318\n",
      "Current step:  100 Loss:  0.005287415922388419\n",
      "Current step:  102 Loss:  0.005175361672427655\n",
      "Current step:  104 Loss:  0.005067844022731682\n",
      "Current step:  106 Loss:  0.004964595846233341\n",
      "Current step:  108 Loss:  0.0048653704570373706\n",
      "Current step:  110 Loss:  0.004769939699660601\n",
      "Current step:  112 Loss:  0.004678092248063689\n",
      "Current step:  114 Loss:  0.0045896320881411235\n",
      "Current step:  116 Loss:  0.0045043771610416014\n",
      "Current step:  118 Loss:  0.004422158147819525\n",
      "Current step:  120 Loss:  0.004342817378569977\n",
      "Current step:  122 Loss:  0.004266207851452692\n",
      "Current step:  124 Loss:  0.004192192348930302\n",
      "Current step:  126 Loss:  0.00412064264018748\n",
      "Current step:  128 Loss:  0.00405143876010338\n",
      "Current step:  130 Loss:  0.003984468356357581\n",
      "Current step:  132 Loss:  0.003919626097290282\n",
      "Current step:  134 Loss:  0.0038568131340352167\n",
      "Current step:  136 Loss:  0.00379593661122101\n",
      "Current step:  138 Loss:  0.0037369092212108084\n",
      "Current step:  140 Loss:  0.003679648797435444\n",
      "Current step:  142 Loss:  0.0036240779428861622\n",
      "Current step:  144 Loss:  0.0035701236902774556\n",
      "Current step:  146 Loss:  0.003517717190781429\n",
      "Current step:  148 Loss:  0.0034667934285748233\n",
      "Current step:  150 Loss:  0.0034172909587408587\n",
      "Current step:  152 Loss:  0.003369151666330778\n",
      "Current step:  154 Loss:  0.0033223205446226223\n",
      "Current step:  156 Loss:  0.0032767454908196577\n",
      "Current step:  158 Loss:  0.0032323771176120007\n",
      "Current step:  160 Loss:  0.0031891685791857432\n",
      "Current step:  162 Loss:  0.00314707541040602\n",
      "Current step:  164 Loss:  0.003106055378027183\n",
      "Current step:  166 Loss:  0.003066068342895383\n",
      "Current step:  168 Loss:  0.0030270761322098274\n",
      "Current step:  170 Loss:  0.0029890424209978086\n",
      "Current step:  172 Loss:  0.0029519326220389225\n",
      "Current step:  174 Loss:  0.0029157137835452273\n",
      "Current step:  176 Loss:  0.002880354493968801\n",
      "Current step:  178 Loss:  0.0028458247933647526\n",
      "Current step:  180 Loss:  0.002812096090790265\n",
      "Current step:  182 Loss:  0.002779141087266417\n",
      "Current step:  184 Loss:  0.0027469337038711528\n",
      "Current step:  186 Loss:  0.0027154490145701498\n",
      "Current step:  188 Loss:  0.0026846631834257307\n",
      "Current step:  190 Loss:  0.002654553405855297\n",
      "Current step:  192 Loss:  0.002625097853638226\n",
      "Current step:  194 Loss:  0.0025962756233958943\n",
      "Current step:  196 Loss:  0.0025680666882917265\n",
      "Current step:  198 Loss:  0.0025404518527195738\n",
      "Current step:  200 Loss:  0.002513412709767268\n",
      "Current step:  202 Loss:  0.0024869316012593673\n",
      "Current step:  204 Loss:  0.0024609915801988196\n",
      "Current step:  206 Loss:  0.0024355763754415225\n",
      "Current step:  208 Loss:  0.0024106703584507345\n",
      "Current step:  210 Loss:  0.0023862585119899986\n",
      "Current step:  212 Loss:  0.002362326400624318\n",
      "Current step:  214 Loss:  0.0023388601429091523\n",
      "Current step:  216 Loss:  0.002315846385155711\n",
      "Current step:  218 Loss:  0.0022932722766695855\n",
      "Current step:  220 Loss:  0.0022711254463671626\n",
      "Current step:  222 Loss:  0.0022493939806815097\n",
      "Current step:  224 Loss:  0.0022280664026754996\n",
      "Current step:  226 Loss:  0.002207131652286129\n",
      "Current step:  228 Loss:  0.002186579067629378\n",
      "Current step:  230 Loss:  0.0021663983672997877\n",
      "Current step:  232 Loss:  0.0021465796336038482\n",
      "Current step:  234 Loss:  0.0021271132966699446\n",
      "Current step:  236 Loss:  0.0021079901193824324\n",
      "Current step:  238 Loss:  0.0020892011830902007\n",
      "Current step:  240 Loss:  0.0020707378740437306\n",
      "Current step:  242 Loss:  0.002052591870517956\n",
      "Current step:  244 Loss:  0.00203475513058068\n",
      "Current step:  246 Loss:  0.002017219880469349\n",
      "Current step:  248 Loss:  0.001999978603540994\n",
      "Current step:  250 Loss:  0.0019830240297630725\n",
      "Current step:  252 Loss:  0.001966349125714137\n",
      "Current step:  254 Loss:  0.0019499470850662015\n",
      "Current step:  256 Loss:  0.0019338113195217638\n",
      "Current step:  258 Loss:  0.0019179354501803188\n",
      "Current step:  260 Loss:  0.0019023132993111766\n",
      "Current step:  262 Loss:  0.0018869388825100172\n",
      "Current step:  264 Loss:  0.0018718064012189608\n",
      "Current step:  266 Loss:  0.0018569102355903686\n",
      "Current step:  268 Loss:  0.0018422449376762234\n",
      "Current step:  270 Loss:  0.0018278052249259143\n",
      "Current step:  272 Loss:  0.0018135859739764003\n",
      "Current step:  274 Loss:  0.0017995822147192327\n",
      "Current step:  276 Loss:  0.0017857891246305004\n",
      "Current step:  278 Loss:  0.0017722020233501747\n",
      "Current step:  280 Loss:  0.0017588163674979226\n",
      "Current step:  282 Loss:  0.0017456277457137083\n",
      "Current step:  284 Loss:  0.001732631873911922\n",
      "Current step:  286 Loss:  0.0017198245907381745\n",
      "Current step:  288 Loss:  0.0017072018532189787\n",
      "Current step:  290 Loss:  0.0016947597325947788\n",
      "Current step:  292 Loss:  0.0016824944103272987\n",
      "Current step:  294 Loss:  0.0016704021742729542\n",
      "Current step:  296 Loss:  0.0016584794150140876\n",
      "Current step:  298 Loss:  0.0016467226223408253\n",
      "Current step:  300 Loss:  0.001635128381876106\n",
      "Current step:  302 Loss:  0.0016236933718372024\n",
      "Current step:  304 Loss:  0.0016124143599275968\n",
      "Current step:  306 Loss:  0.0016012882003527398\n",
      "Current step:  308 Loss:  0.0015903118309543115\n",
      "Current step:  310 Loss:  0.0015794822704573938\n",
      "Current step:  312 Loss:  0.0015687966158253646\n",
      "Current step:  314 Loss:  0.0015582520397178046\n",
      "Current step:  316 Loss:  0.0015478457880465472\n",
      "Current step:  318 Loss:  0.001537575177625689\n",
      "Current step:  320 Loss:  0.0015274375939113138\n",
      "Current step:  322 Loss:  0.00151743048882683\n",
      "Current step:  324 Loss:  0.0015075513786704513\n",
      "Current step:  326 Loss:  0.0014977978421009223\n",
      "Current step:  328 Loss:  0.0014881675181983548\n",
      "Current step:  330 Loss:  0.0014786581045967246\n",
      "Current step:  332 Loss:  0.0014692673556851745\n",
      "Current step:  334 Loss:  0.0014599930808749432\n",
      "Current step:  336 Loss:  0.0014508331429294277\n",
      "Current step:  338 Loss:  0.001441785456354434\n",
      "Current step:  340 Loss:  0.0014328479858463409\n",
      "Current step:  342 Loss:  0.0014240187447955976\n",
      "Current step:  344 Loss:  0.0014152957938433778\n",
      "Current step:  346 Loss:  0.0014066772394891688\n",
      "Current step:  348 Loss:  0.0013981612327471343\n",
      "Current step:  350 Loss:  0.001389745967849459\n",
      "Current step:  352 Loss:  0.0013814296809945515\n",
      "Current step:  354 Loss:  0.0013732106491385165\n",
      "Current step:  356 Loss:  0.0013650871888279776\n",
      "Current step:  358 Loss:  0.001357057655072731\n",
      "Current step:  360 Loss:  0.001349120440256651\n",
      "Current step:  362 Loss:  0.001341273973085302\n",
      "Current step:  364 Loss:  0.0013335167175687942\n",
      "Current step:  366 Loss:  0.0013258471720385902\n",
      "Current step:  368 Loss:  0.0013182638681969979\n",
      "Current step:  370 Loss:  0.0013107653701978517\n",
      "Current step:  372 Loss:  0.0013033502737574768\n",
      "Current step:  374 Loss:  0.001296017205294652\n",
      "Current step:  376 Loss:  0.0012887648210984362\n",
      "Current step:  378 Loss:  0.0012815918065228853\n",
      "Current step:  380 Loss:  0.0012744968752076513\n",
      "Current step:  382 Loss:  0.0012674787683234208\n",
      "Current step:  384 Loss:  0.001260536253841398\n",
      "Current step:  386 Loss:  0.0012536681258258366\n",
      "Current step:  388 Loss:  0.0012468732037488306\n",
      "Current step:  390 Loss:  0.0012401503318265126\n",
      "Current step:  392 Loss:  0.0012334983783760054\n",
      "Current step:  394 Loss:  0.001226916235192142\n",
      "Current step:  396 Loss:  0.0012204028169435339\n",
      "Current step:  398 Loss:  0.0012139570605870266\n",
      "Current step:  400 Loss:  0.001207577924800111\n",
      "Current step:  402 Loss:  0.0012012643894305\n",
      "Current step:  404 Loss:  0.0011950154549623365\n",
      "Current step:  406 Loss:  0.0011888301419984465\n",
      "Current step:  408 Loss:  0.0011827074907580842\n",
      "Current step:  410 Loss:  0.001176646560589595\n",
      "Current step:  412 Loss:  0.0011706464294975083\n",
      "Current step:  414 Loss:  0.0011647061936835522\n",
      "Current step:  416 Loss:  0.0011588249671012135\n",
      "Current step:  418 Loss:  0.0011530018810231506\n",
      "Current step:  420 Loss:  0.001147236083621274\n",
      "Current step:  422 Loss:  0.001141526739558894\n",
      "Current step:  424 Loss:  0.00113587302959471\n",
      "Current step:  426 Loss:  0.0011302741501979898\n",
      "Current step:  428 Loss:  0.0011247293131748364\n",
      "Current step:  430 Loss:  0.0011192377453049362\n",
      "Current step:  432 Loss:  0.001113798687988693\n",
      "Current step:  434 Loss:  0.0011084113969042054\n",
      "Current step:  436 Loss:  0.001103075141673795\n",
      "Current step:  438 Loss:  0.0010977892055399098\n",
      "Current step:  440 Loss:  0.0010925528850499438\n",
      "Current step:  442 Loss:  0.001087365489749777\n",
      "Current step:  444 Loss:  0.0010822263418857176\n",
      "Current step:  446 Loss:  0.001077134776114646\n",
      "Current step:  448 Loss:  0.0010720901392219195\n",
      "Current step:  450 Loss:  0.001067091789847086\n",
      "Current step:  452 Loss:  0.001062139098216845\n",
      "Current step:  454 Loss:  0.001057231445885287\n",
      "Current step:  456 Loss:  0.0010523682254810157\n",
      "Current step:  458 Loss:  0.0010475488404610325\n",
      "Current step:  460 Loss:  0.001042772704871131\n",
      "Current step:  462 Loss:  0.0010380392431126064\n",
      "Current step:  464 Loss:  0.0010333478897150829\n",
      "Current step:  466 Loss:  0.0010286980891154028\n",
      "Current step:  468 Loss:  0.0010240892954419844\n",
      "Current step:  470 Loss:  0.0010195209723051814\n",
      "Current step:  472 Loss:  0.00101499259259267\n",
      "Current step:  474 Loss:  0.0010105036382703357\n",
      "Current step:  476 Loss:  0.0010060536001881785\n",
      "Current step:  478 Loss:  0.001001641977891183\n",
      "Current step:  480 Loss:  0.000997268279434947\n",
      "Current step:  482 Loss:  0.0009929320212060527\n",
      "Current step:  484 Loss:  0.0009886327277469054\n",
      "Current step:  486 Loss:  0.0009843699315849187\n",
      "Current step:  488 Loss:  0.0009801431730661276\n",
      "Current step:  490 Loss:  0.0009759520001927398\n",
      "Current step:  492 Loss:  0.0009717959684648321\n",
      "Current step:  494 Loss:  0.0009676746407259807\n",
      "Current step:  496 Loss:  0.000963587587012511\n",
      "Current step:  498 Loss:  0.0009595343844066897\n",
      "Current step:  500 Loss:  0.0009555146168932226\n",
      "Current step:  502 Loss:  0.000951527875219505\n",
      "Current step:  504 Loss:  0.000947573756758984\n",
      "Current step:  506 Loss:  0.0009436518653780533\n",
      "Current step:  508 Loss:  0.0009397618113059609\n",
      "Current step:  510 Loss:  0.0009359032110079166\n",
      "Current step:  512 Loss:  0.0009320756870612142\n",
      "Current step:  514 Loss:  0.0009282788680342569\n",
      "Current step:  516 Loss:  0.0009245123883684227\n",
      "Current step:  518 Loss:  0.0009207758882628049\n",
      "Current step:  520 Loss:  0.0009170690135615325\n",
      "Current step:  522 Loss:  0.0009133914156438185\n",
      "Current step:  524 Loss:  0.0009097427513164711\n",
      "Current step:  526 Loss:  0.0009061226827090037\n",
      "Current step:  528 Loss:  0.0009025308771710744\n",
      "Current step:  530 Loss:  0.0008989670071722969\n",
      "Current step:  532 Loss:  0.0008954307502043856\n",
      "Current step:  534 Loss:  0.0008919217886855491\n",
      "Current step:  536 Loss:  0.0008884398098669166\n",
      "Current step:  538 Loss:  0.0008849845057412854\n",
      "Current step:  540 Loss:  0.0008815555729537738\n",
      "Current step:  542 Loss:  0.0008781527127145407\n",
      "Current step:  544 Loss:  0.0008747756307134651\n",
      "Current step:  546 Loss:  0.0008714240370366989\n",
      "Current step:  548 Loss:  0.0008680976460850733\n",
      "Current step:  550 Loss:  0.0008647961764944131\n",
      "Current step:  552 Loss:  0.0008615193510574087\n",
      "Current step:  554 Loss:  0.0008582668966474205\n",
      "Current step:  556 Loss:  0.000855038544143793\n",
      "Current step:  558 Loss:  0.0008518340283588989\n",
      "Current step:  560 Loss:  0.000848653087966711\n",
      "Current step:  562 Loss:  0.0008454954654329488\n",
      "Current step:  564 Loss:  0.0008423609069467204\n",
      "Current step:  566 Loss:  0.0008392491623536525\n",
      "Current step:  568 Loss:  0.0008361599850904337\n",
      "Current step:  570 Loss:  0.0008330931321207807\n",
      "Current step:  572 Loss:  0.0008300483638727335\n",
      "Current step:  574 Loss:  0.0008270254441773026\n",
      "Current step:  576 Loss:  0.000824024140208439\n",
      "Current step:  578 Loss:  0.0008210442224241994\n",
      "Current step:  580 Loss:  0.0008180854645091992\n",
      "Current step:  582 Loss:  0.0008151476433182907\n",
      "Current step:  584 Loss:  0.0008122305388213332\n",
      "Current step:  586 Loss:  0.000809333934049195\n",
      "Current step:  588 Loss:  0.0008064576150408273\n",
      "Current step:  590 Loss:  0.0008036013707914338\n",
      "Current step:  592 Loss:  0.0008007649932017412\n",
      "Current step:  594 Loss:  0.0007979482770282306\n",
      "Current step:  596 Loss:  0.0007951510198345148\n",
      "Current step:  598 Loss:  0.0007923730219435307\n",
      "Current step:  600 Loss:  0.0007896140863908952\n",
      "Current step:  602 Loss:  0.0007868740188790384\n",
      "Current step:  604 Loss:  0.0007841526277323676\n",
      "Current step:  606 Loss:  0.0007814497238532807\n",
      "Current step:  608 Loss:  0.0007787651206790765\n",
      "Current step:  610 Loss:  0.0007760986341397305\n",
      "Current step:  612 Loss:  0.0007734500826164909\n",
      "Current step:  614 Loss:  0.0007708192869013168\n",
      "Current step:  616 Loss:  0.0007682060701571269\n",
      "Current step:  618 Loss:  0.0007656102578787656\n",
      "Current step:  620 Loss:  0.0007630316778548159\n",
      "Current step:  622 Loss:  0.000760470160130155\n",
      "Current step:  624 Loss:  0.0007579255369691412\n",
      "Current step:  626 Loss:  0.0007553976428196338\n",
      "Current step:  628 Loss:  0.0007528863142776852\n",
      "Current step:  630 Loss:  0.0007503913900528388\n",
      "Current step:  632 Loss:  0.0007479127109341698\n",
      "Current step:  634 Loss:  0.000745450119757\n",
      "Current step:  636 Loss:  0.0007430034613701551\n",
      "Current step:  638 Loss:  0.0007405725826038962\n",
      "Current step:  640 Loss:  0.0007381573322385394\n",
      "Current step:  642 Loss:  0.00073575756097349\n",
      "Current step:  644 Loss:  0.000733373121397064\n",
      "Current step:  646 Loss:  0.000731003867956715\n",
      "Current step:  648 Loss:  0.0007286496569299111\n",
      "Current step:  650 Loss:  0.0007263103463955394\n",
      "Current step:  652 Loss:  0.0007239857962058371\n",
      "Current step:  654 Loss:  0.0007216758679588031\n",
      "Current step:  656 Loss:  0.0007193804249712526\n",
      "Current step:  658 Loss:  0.0007170993322521931\n",
      "Current step:  660 Loss:  0.0007148324564768022\n",
      "Current step:  662 Loss:  0.0007125796659609089\n",
      "Current step:  664 Loss:  0.0007103408306358203\n",
      "Current step:  666 Loss:  0.0007081158220237584\n",
      "Current step:  668 Loss:  0.0007059045132135905\n",
      "Current step:  670 Loss:  0.0007037067788371378\n",
      "Current step:  672 Loss:  0.0007015224950458387\n",
      "Current step:  674 Loss:  0.0006993515394878273\n",
      "Current step:  676 Loss:  0.000697193791285469\n",
      "Current step:  678 Loss:  0.0006950491310132548\n",
      "Current step:  680 Loss:  0.0006929174406761355\n",
      "Current step:  682 Loss:  0.0006907986036882028\n",
      "Current step:  684 Loss:  0.0006886925048517746\n",
      "Current step:  686 Loss:  0.0006865990303368142\n",
      "Current step:  688 Loss:  0.0006845180676607919\n",
      "Current step:  690 Loss:  0.0006824495056688095\n",
      "Current step:  692 Loss:  0.0006803932345141378\n",
      "Current step:  694 Loss:  0.0006783491456390386\n",
      "Current step:  696 Loss:  0.000676317131756025\n",
      "Current step:  698 Loss:  0.0006742970868293034\n",
      "Current step:  700 Loss:  0.0006722889060566519\n",
      "Current step:  702 Loss:  0.0006702924858515986\n",
      "Current step:  704 Loss:  0.0006683077238258401\n",
      "Current step:  706 Loss:  0.0006663345187720311\n",
      "Current step:  708 Loss:  0.0006643727706468317\n",
      "Current step:  710 Loss:  0.0006624223805542815\n",
      "Current step:  712 Loss:  0.0006604832507294165\n",
      "Current step:  714 Loss:  0.0006585552845222084\n",
      "Current step:  716 Loss:  0.0006566383863817089\n",
      "Current step:  718 Loss:  0.0006547324618405511\n",
      "Current step:  720 Loss:  0.000652837417499651\n",
      "Current step:  722 Loss:  0.0006509531610131859\n",
      "Current step:  724 Loss:  0.0006490796010738193\n",
      "Current step:  726 Loss:  0.0006472166473981638\n",
      "Current step:  728 Loss:  0.0006453642107125469\n",
      "Current step:  730 Loss:  0.0006435222027389093\n",
      "Current step:  732 Loss:  0.0006416905361810413\n",
      "Current step:  734 Loss:  0.0006398691247109709\n",
      "Current step:  736 Loss:  0.0006380578829556251\n",
      "Current step:  738 Loss:  0.0006362567264836977\n",
      "Current step:  740 Loss:  0.0006344655717926983\n",
      "Current step:  742 Loss:  0.0006326843362962913\n",
      "Current step:  744 Loss:  0.0006309129383117325\n",
      "Current step:  746 Loss:  0.0006291512970476498\n",
      "Current step:  748 Loss:  0.000627399332591879\n",
      "Current step:  750 Loss:  0.0006256569658995973\n",
      "Current step:  752 Loss:  0.000623924118781598\n",
      "Current step:  754 Loss:  0.0006222007138927824\n",
      "Current step:  756 Loss:  0.0006204866747208389\n",
      "Current step:  758 Loss:  0.0006187819255750372\n",
      "Current step:  760 Loss:  0.0006170863915753197\n",
      "Current step:  762 Loss:  0.0006153999986414642\n",
      "Current step:  764 Loss:  0.00061372267348247\n",
      "Current step:  766 Loss:  0.0006120543435860879\n",
      "Current step:  768 Loss:  0.0006103949372085395\n",
      "Current step:  770 Loss:  0.0006087443833643702\n",
      "Current step:  772 Loss:  0.0006071026118165321\n",
      "Current step:  774 Loss:  0.0006054695530664691\n",
      "Current step:  776 Loss:  0.0006038451383445628\n",
      "Current step:  778 Loss:  0.000602229299600563\n",
      "Current step:  780 Loss:  0.0006006219694942378\n",
      "Current step:  782 Loss:  0.0005990230813861544\n",
      "Current step:  784 Loss:  0.0005974325693286296\n",
      "Current step:  786 Loss:  0.0005958503680567772\n",
      "Current step:  788 Loss:  0.0005942764129797033\n",
      "Current step:  790 Loss:  0.0005927106401718868\n",
      "Current step:  792 Loss:  0.0005911529863646014\n",
      "Current step:  794 Loss:  0.000589603388937575\n",
      "Current step:  796 Loss:  0.0005880617859106845\n",
      "Current step:  798 Loss:  0.0005865281159358028\n",
      "Current step:  800 Loss:  0.0005850023182888337\n",
      "Current step:  802 Loss:  0.0005834843328617818\n",
      "Current step:  804 Loss:  0.0005819741001549585\n",
      "Current step:  806 Loss:  0.0005804715612693727\n",
      "Current step:  808 Loss:  0.0005789766578991367\n",
      "Current step:  810 Loss:  0.0005774893323240605\n",
      "Current step:  812 Loss:  0.0005760095274023316\n",
      "Current step:  814 Loss:  0.0005745371865633161\n",
      "Current step:  816 Loss:  0.0005730722538004041\n",
      "Current step:  818 Loss:  0.0005716146736640871\n",
      "Current step:  820 Loss:  0.0005701643912550117\n",
      "Current step:  822 Loss:  0.000568721352217213\n",
      "Current step:  824 Loss:  0.0005672855027313875\n",
      "Current step:  826 Loss:  0.0005658567895083696\n",
      "Current step:  828 Loss:  0.0005644351597825359\n",
      "Current step:  830 Loss:  0.000563020561305529\n",
      "Current step:  832 Loss:  0.0005616129423398285\n",
      "Current step:  834 Loss:  0.0005602122516525983\n",
      "Current step:  836 Loss:  0.0005588184385095742\n",
      "Current step:  838 Loss:  0.0005574314526689797\n",
      "Current step:  840 Loss:  0.0005560512443756287\n",
      "Current step:  842 Loss:  0.0005546777643550163\n",
      "Current step:  844 Loss:  0.0005533109638075723\n",
      "Current step:  846 Loss:  0.0005519507944029417\n",
      "Current step:  848 Loss:  0.0005505972082743946\n",
      "Current step:  850 Loss:  0.000549250158013283\n",
      "Current step:  852 Loss:  0.0005479095966635781\n",
      "Current step:  854 Loss:  0.0005465754777164927\n",
      "Current step:  856 Loss:  0.0005452477551052005\n",
      "Current step:  858 Loss:  0.0005439263831995968\n",
      "Current step:  860 Loss:  0.0005426113168011282\n",
      "Current step:  862 Loss:  0.0005413025111377554\n",
      "Current step:  864 Loss:  0.0005399999218589141\n",
      "Current step:  866 Loss:  0.0005387035050305834\n",
      "Current step:  868 Loss:  0.0005374132171304164\n",
      "Current step:  870 Loss:  0.0005361290150429557\n",
      "Current step:  872 Loss:  0.0005348508560548836\n",
      "Current step:  874 Loss:  0.0005335786978503574\n",
      "Current step:  876 Loss:  0.0005323124985063925\n",
      "Current step:  878 Loss:  0.0005310522164883819\n",
      "Current step:  880 Loss:  0.0005297978106455287\n",
      "Current step:  882 Loss:  0.0005285492402065093\n",
      "Current step:  884 Loss:  0.0005273064647750916\n",
      "Current step:  886 Loss:  0.0005260694443258183\n",
      "Current step:  888 Loss:  0.0005248381391998004\n",
      "Current step:  890 Loss:  0.0005236125101005433\n",
      "Current step:  892 Loss:  0.0005223925180898052\n",
      "Current step:  894 Loss:  0.0005211781245835422\n",
      "Current step:  896 Loss:  0.000519969291347889\n",
      "Current step:  898 Loss:  0.0005187659804952347\n",
      "Current step:  900 Loss:  0.0005175681544802769\n",
      "Current step:  902 Loss:  0.0005163757760961985\n",
      "Current step:  904 Loss:  0.0005151888084708758\n",
      "Current step:  906 Loss:  0.0005140072150631076\n",
      "Current step:  908 Loss:  0.0005128309596589425\n",
      "Current step:  910 Loss:  0.0005116600063680186\n",
      "Current step:  912 Loss:  0.0005104943196199775\n",
      "Current step:  914 Loss:  0.0005093338641608848\n",
      "Current step:  916 Loss:  0.000508178605049754\n",
      "Current step:  918 Loss:  0.0005070285076551034\n",
      "Current step:  920 Loss:  0.0005058835376514743\n",
      "Current step:  922 Loss:  0.0005047436610161611\n",
      "Current step:  924 Loss:  0.0005036088440258139\n",
      "Current step:  926 Loss:  0.0005024790532531848\n",
      "Current step:  928 Loss:  0.0005013542555639095\n",
      "Current step:  930 Loss:  0.0005002344181132851\n",
      "Current step:  932 Loss:  0.0004991195083431242\n",
      "Current step:  934 Loss:  0.0004980094939786727\n",
      "Current step:  936 Loss:  0.0004969043430255041\n",
      "Current step:  938 Loss:  0.0004958040237664858\n",
      "Current step:  940 Loss:  0.0004947085047588282\n",
      "Current step:  942 Loss:  0.0004936177548310927\n",
      "Current step:  944 Loss:  0.000492531743080299\n",
      "Current step:  946 Loss:  0.000491450438869036\n",
      "Current step:  948 Loss:  0.000490373811822619\n",
      "Current step:  950 Loss:  0.0004893018318262942\n",
      "Current step:  952 Loss:  0.00048823446902246414\n",
      "Current step:  954 Loss:  0.00048717169380797017\n",
      "Current step:  956 Loss:  0.00048611347683135894\n",
      "Current step:  958 Loss:  0.0004850597889902484\n",
      "Current step:  960 Loss:  0.00048401060142870723\n",
      "Current step:  962 Loss:  0.00048296588553460487\n",
      "Current step:  964 Loss:  0.0004819256129371093\n",
      "Current step:  966 Loss:  0.0004808897555041026\n",
      "Current step:  968 Loss:  0.0004798582853397218\n",
      "Current step:  970 Loss:  0.0004788311747818617\n",
      "Current step:  972 Loss:  0.0004778083963997543\n",
      "Current step:  974 Loss:  0.00047678992299153924\n",
      "Current step:  976 Loss:  0.000475775727581909\n",
      "Current step:  978 Loss:  0.0004747657834197483\n",
      "Current step:  980 Loss:  0.0004737600639758131\n",
      "Current step:  982 Loss:  0.00047275854294045745\n",
      "Current step:  984 Loss:  0.0004717611942213274\n",
      "Current step:  986 Loss:  0.0004707679919411882\n",
      "Current step:  988 Loss:  0.0004697789104356645\n",
      "Current step:  990 Loss:  0.0004687939242510981\n",
      "Current step:  992 Loss:  0.0004678130081423438\n",
      "Current step:  994 Loss:  0.00046683613707072206\n",
      "Current step:  996 Loss:  0.00046586328620183877\n",
      "Current step:  998 Loss:  0.00046489443090355924\n",
      "[Value(data=0.9920687382052715), Value(data=-0.9920858581792807), Value(data=-0.9881691234740339), Value(data=0.9859137885520525)]\n"
     ]
    }
   ],
   "source": [
    "# There is a very common bug in the code above. We are not setting the gradients back to zero. lol\n",
    "# The only reason previous one was working because the data is very simple.\n",
    "# Also you can see the number of epochs has increased by a lot. Gradients are being set back to zero.\n",
    "# Previously they were being added.\n",
    "# Let's fix that!\n",
    "\n",
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "# We are doing gradient descent here\n",
    "for e in range(1000):\n",
    "    # forward pass\n",
    "    ypred =[n(x) for x in xs]\n",
    "    loss = sum([(yout - ygt)**2 for ygt, yout in zip(ys, ypred)])\n",
    "    \n",
    "    # zero grad\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0      # we need to set the gradients back to zero, otherwise they will accumulate\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -learning_rate * p.grad\n",
    "    \n",
    "    # print every\n",
    "    if e % 2 == 0:\n",
    "        print(\"Current step: \", e, \"Loss: \", loss.data)\n",
    "\n",
    "# After last step of training let's predict again\n",
    "ypred =[n(x) for x in xs]\n",
    "print(ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deeply')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ba283c99aab3f9826c9057a6b77e5ed1375b1012fccd26237ed2bb4d681a306"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
